{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4842985-b07d-4b73-b7e8-1f019d485691",
   "metadata": {},
   "source": [
    "## Estimate the stan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c94e29-19f9-4783-8250-2463f77f6e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cmdstanpy\n",
    "from cmdstanpy import CmdStanModel\n",
    "import arviz\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import ast\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0463101-a8ed-4ad2-a2f5-efc960944bb2",
   "metadata": {},
   "source": [
    "#### Choose which model specification to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f0d9d-6b5b-46de-be93-b696efb50130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type='beta'  # change this to run the robustness tests with different specifications\n",
    "stan_path='stan_specification/'\n",
    "\n",
    "if model_type=='nonlinear':\n",
    "    model = CmdStanModel(stan_file=stan_path+'nonlinear.stan')\n",
    "    seed = 1\n",
    "    output_folder='nonlinear/'\n",
    "    coefficient_dict={'intercept':'beta1','sndi':'beta2','density':'beta3','precip':'beta4','min_temp':'beta5',\n",
    "                    'bikelanes':'beta6','slope':'beta7','includes_inboundoutbound':'beta8','rail_in_city':'beta9','max_temp':'beta10','population':'beta11',\n",
    "                    'min_temp2':'beta12','max_temp2':'beta13','density2':'beta14'}\n",
    "\n",
    "\n",
    "if model_type in['linear','high_gdp','beta','trips']:\n",
    "    if model_type=='linear':\n",
    "        model = CmdStanModel(stan_file=stan_path+'linear.stan')\n",
    "    else:\n",
    "        model = CmdStanModel(stan_file=stan_path+'beta.stan')\n",
    "    seed = 1 \n",
    "    output_folder=model_type+'/'\n",
    "    coefficient_dict={'intercept':'beta1','sndi':'beta2','density':'beta3','precip':'beta4','min_temp':'beta5',\n",
    "                      'bikelanes':'beta6','slope':'beta7','includes_inboundoutbound':'beta8','rail_in_city':'beta9','max_temp':'beta10','population':'beta11',\n",
    "                      'min_temp2':'beta12'}\n",
    "\n",
    "if model_type=='inoutbound_only':\n",
    "    model = CmdStanModel(stan_file=stan_path+'inoutbound_only.stan') #  same as beta with one less variable\n",
    "    seed = 2 # for seed 1, chains diverge\n",
    "    output_folder='inoutbound_only/'\n",
    "    coefficient_dict={'intercept':'beta1','sndi':'beta2','density':'beta3','precip':'beta4','min_temp':'beta5',\n",
    "                      'bikelanes':'beta6','slope':'beta7','rail_in_city':'beta8','max_temp':'beta9','population':'beta10',\n",
    "                      'min_temp2':'beta11'} # same as beta but drop includes_inboundoutbound\n",
    "    \n",
    "if model_type== 'saturated':\n",
    "    model = CmdStanModel(stan_file=stan_path+'saturated.stan')\n",
    "    seed = 1\n",
    "    output_folder='saturated/'\n",
    "    coefficient_dict={'intercept':'beta1','sndi':'beta2','density':'beta3','precip':'beta4','min_temp':'beta5',\n",
    "                'bikelanes':'beta6','slope':'beta7','includes_inboundoutbound':'beta8','rail_in_city':'beta9','max_temp':'beta10',\n",
    "                'sndi_added':'beta11','population':'beta12','motorways':'beta13'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b0a7f-d9f0-47dd-8d53-3e3f22b48eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify dependent variables and column names in the dataframe\n",
    "if model_type=='trips':\n",
    "    y_bike_name='tripshare_cycling_23'\n",
    "    y_walk_name='tripshare_on_foot_23' \n",
    "else:\n",
    "    y_bike_name='km_share_cycling_no_transit'\n",
    "    y_walk_name='km_share_on_foot_no_transit'\n",
    "\n",
    "# calculate the number of city-level variables\n",
    "num_city_x=len(coefficient_dict.keys())-1\n",
    "city_var_order={key: index for index, key in enumerate(coefficient_dict.keys())}\n",
    "\n",
    "# Create arrays of city-level variables\n",
    "city_data=pd.read_csv('data/data_23.csv')\n",
    "\n",
    "# drop state column first, because we don't use it, and it has a lot of missing data\n",
    "city_data = city_data.drop('state',axis=1).dropna()\n",
    "\n",
    "# Calculate country-level means, in order to do marginal effects\n",
    "vlist = [vv+'_standard' for vv in coefficient_dict.keys() if vv+'_standard' in city_data.columns]\n",
    "country_means = city_data.groupby('country_num')[vlist].mean()\n",
    "\n",
    "# add squared values\n",
    "for col in coefficient_dict:\n",
    "    if col.endswith('2'):\n",
    "        country_means[col+'_standard'] = country_means[col[:-1]+'_standard'] **2\n",
    "# add binary variables\n",
    "country_means = country_means.join(city_data.groupby('country_num')[['includes_inboundoutbound','rail_in_city']].mean())\n",
    "\n",
    "country_means.sort_index(inplace=True) # sort by country_num, which is the index\n",
    "\n",
    "if model_type=='inoutbound_only':\n",
    "    city_data = city_data[city_data.includes_inboundoutbound==1].reset_index()\n",
    "if model_type=='high_gdp':\n",
    "    gdp_cutoff = city_data.groupby('country_num').gdp_standard.mean().median()  # mean to get country-level GDP, then take median\n",
    "    city_data = city_data[city_data.gdp_standard>=gdp_cutoff].reset_index()\n",
    "\n",
    "# dependent variables\n",
    "y_bike = city_data[y_bike_name]\n",
    "y_walk = city_data[y_walk_name]\n",
    "\n",
    "# independent variables\n",
    "country_num = city_data.country_num.values\n",
    "sndi_standard = city_data.sndi_standard.values\n",
    "density_standard = city_data.density_standard.values\n",
    "density_standard2 = city_data.density_standard2.values\n",
    "precip_standard = city_data.precip_standard.values\n",
    "min_temp_standard = city_data.min_temp_standard.values\n",
    "min_temp_standard2 = city_data.min_temp_standard2.values\n",
    "max_temp_standard = city_data.max_temp_standard.values\n",
    "max_temp_standard2 = city_data.max_temp_standard2.values\n",
    "bikelanes_standard = city_data.bikelanes_standard.values\n",
    "slope_standard = city_data.slope_standard.values\n",
    "sndi_added=city_data.sndi_added_standard.values\n",
    "population=city_data.population_standard.values\n",
    "includes_inboundoutbound=city_data.includes_inboundoutbound.values\n",
    "motorways_standard=city_data.motorways_standard.values\n",
    "rail_in_city=city_data.rail_in_city.values\n",
    "n_country = city_data.groupby('country_num')['feature_id'].count()\n",
    "\n",
    "# create arrays from country-level variables\n",
    "country_data=pd.read_csv('data/country_23.csv')\n",
    "gdp_standard = country_data.gdp_standard.values\n",
    "dependency_standard = country_data.dependency_standard.values\n",
    "gasoline_standard=country_data.gasoline_standard.values\n",
    "country_num_alpha=country_data.index.values\n",
    "\n",
    "if model_type!='linear':\n",
    "    # beta model can't accept values of 0 so we make all of these 0.0001\n",
    "    city_data['bike_share2']=city_data[y_bike_name].apply(lambda x: x if x>0 else 0.0001)\n",
    "    y_bike=city_data.bike_share2.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ae578-e677-465a-892c-7ac8e83ca462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hierarchical_data = {'N': len(y_bike),\n",
    "                          'J': len(gdp_standard),\n",
    "                          'L' : num_city_x,\n",
    "                          'M' : 3,   \n",
    "                          'country_num': country_num+1, # Stan counts starting at 1\n",
    "                          'gdp_percapita': gdp_standard,\n",
    "                          'dependency_ratio':dependency_standard,\n",
    "                          'gas_prices':gasoline_standard,\n",
    "                          'sndi': sndi_standard,\n",
    "                          'density': density_standard,\n",
    "                          'density2': density_standard2,\n",
    "                          'precip': precip_standard,\n",
    "                          'min_temp': min_temp_standard,\n",
    "                          'min_temp2':min_temp_standard2,\n",
    "                          'max_temp': max_temp_standard,\n",
    "                          'max_temp2': max_temp_standard2,\n",
    "                          'bikelanes':bikelanes_standard,\n",
    "                          'slope':slope_standard,\n",
    "                          'sndi_added':sndi_added,\n",
    "                          'population':population,\n",
    "                          'includes_inboundoutbound':includes_inboundoutbound.astype(int),\n",
    "                          'motorways':motorways_standard,\n",
    "                          'rail_in_city':rail_in_city.astype(int),\n",
    "                          'y_bike': y_bike,\n",
    "                          'y_walk': y_walk,\n",
    "                          'bikelanes_global_mean':city_data.bikelane_per_road_km.mean(),\n",
    "                          'bikelanes_global_stdev':np.std(city_data.bikelane_per_road_km)}\n",
    "\n",
    "# add group-level means\n",
    "for col in country_means.columns:\n",
    "        hierarchical_data[col.replace('_standard','')+'_mean'] = country_means[col].values\n",
    "      \n",
    "\n",
    "# read in fit or fit the model\n",
    "# change path to where you want the (very large) stan output files to reside\n",
    "outputpath = '../stan_output/'\n",
    "if not(os.path.exists(outputpath)): os.mkdir(outputpath)\n",
    "    \n",
    "# if you want to read in previous estimates\n",
    "#combined_fit=cmdstanpy.from_csv(path=outputpath, method=None)\n",
    "\n",
    "# if you want to run the model. this will take a few hours\n",
    "combined_fit = model.sample(data=hierarchical_data, inits=0.5, seed=seed, iter_warmup=1000, iter_sampling=1000, max_treedepth=15, adapt_delta=0.9,output_dir=outputpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7bbe6-5bd4-4f98-9e04-b71be6255427",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Basic Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165e7f7-d933-438b-ae6d-2e63e7c0fd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(combined_fit.diagnose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4581cd00-1f22-4608-936d-e863a618d01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see https://mc-stan.org/docs/2_27/stan-users-guide/posterior-p-values.html\n",
    "fig, axes = plt.subplots(1,2, figsize=(7, 3))\n",
    "for mode, yvar, ax in zip(['walk','bike'],[y_walk,y_bike], axes):\n",
    "    forarviz = arviz.from_cmdstanpy(\n",
    "        posterior=combined_fit,\n",
    "        predictions = 'y_hat_'+mode,\n",
    "        posterior_predictive='y_rep_'+mode, \n",
    "        observed_data={\"y\": yvar})\n",
    "\n",
    "\n",
    "    print('% of samples for {} that are greater than mean: {:.1f}'.format(mode, 100*forarviz.posterior['mean_gt_'+mode].mean().values))\n",
    "    print('Should be about 50%')\n",
    "    arviz.plot_bpv(forarviz, kind=\"p_value\", data_pairs = {'y' : 'y_rep_'+mode}) \n",
    "    arviz.plot_ppc(forarviz, data_pairs = {'y' : 'y_rep_'+mode}, ax=ax) \n",
    "    ax.set_xlabel('{} mode share'.format(mode.title()), size=11)\n",
    "    ax.set_xlim(-0.1,0.3)\n",
    "    ax.legend(loc=1, prop={'size': 9})\n",
    "    ax.set_xticks([0,0.1, 0.2, 0.3])\n",
    "    ax.tick_params(axis='x', which='major', labelsize=10)\n",
    "\n",
    "    vnames = ['tau_'+mode,'gamma_'+mode] +['phi_'+mode]*('beta' in model_type)\n",
    "    arviz.plot_trace(forarviz, var_names=vnames)\n",
    "\n",
    "fig.tight_layout()\n",
    "if not(os.path.exists(outputpath)): os.mkdir(outputpath)\n",
    "fig.savefig(outputpath+'ppc_{}.png'.format(model_type), dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fb692-2f79-4243-8c5b-5d600d601d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the summary for all variables\n",
    "combined_summary=combined_fit.summary(percentiles=(5, 50, 95), sig_figs=6)\n",
    "combined_summary.reset_index(names=['var_name'],inplace=True)\n",
    "def extract_values(row):\n",
    "    return ast.literal_eval(row)[0], ast.literal_eval(row)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9839d-bc03-401d-9867-4fc6595b4fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhats=pd.DataFrame()\n",
    "for mode in ['walk', 'bike']:\n",
    "    # filter to just estimates of betas (city-level predictors)\n",
    "    yhat_summary=combined_summary[combined_summary.var_name.str.contains(f'y_hat_{mode}')].copy()\n",
    "    yhat_summary.var_name = yhat_summary.var_name.apply(lambda x: x.replace('mu_gen','y_hat'))\n",
    "    # unpack the city\n",
    "    yhat_summary.var_name=yhat_summary.var_name.str.replace(f'y_hat_{mode}','')\n",
    "    yhat_summary['city']=yhat_summary['var_name'].apply(lambda x: ast.literal_eval(x)[0]-1)\n",
    "    yhat_summary.reset_index(drop=True)\n",
    "    yhat_summary.rename(columns={'Mean':f'y_hat_mean_{mode}'},inplace=True)\n",
    "    yhats=pd.concat([yhats,yhat_summary.set_index('city')[[f'y_hat_mean_{mode}']]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33a1c3-7501-4689-8ece-5642a32b27ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create prediction df with y_hats and difference between actual and expected modeshare\n",
    "prediction_df=city_data.join(yhats)\n",
    "prediction_df.plot(kind='scatter',y='y_hat_mean_bike',x=y_bike_name,s=0.5)\n",
    "prediction_df.plot(kind='scatter',y='y_hat_mean_walk',x=y_walk_name,s=0.5)\n",
    "\n",
    "prediction_df.to_csv(outputpath+'cmdstan_'+model_type+'_prediction_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526b3b1-6186-409f-8490-533e5240ed88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate mean squared error\n",
    "plt.close('all')\n",
    "MSE_walk=sklearn.metrics.mean_squared_error(prediction_df[y_walk_name], prediction_df['y_hat_mean_walk'])\n",
    "MSE_bike=sklearn.metrics.mean_squared_error(prediction_df[y_bike_name], prediction_df['y_hat_mean_bike'])\n",
    "pd.DataFrame([MSE_walk,MSE_bike],columns=['MSE'],index=['walk', 'bike']).to_csv(outputpath+'MSE_'+model_type+'.csv')\n",
    "print(MSE_walk,MSE_bike)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bef4a-41ef-49eb-b95b-c44005fe1517",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Trace plots (for convergence check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d27af-5ca1-4d78-bfc9-0835ed0011ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reformat fit.stan_variables()['beta'] to a set of betas that are plottable in arviz\n",
    "betas = {}\n",
    "\n",
    "betas['betas_bike']=combined_fit.stan_variables()['beta_bike']\n",
    "betas['betas_walk']=combined_fit.stan_variables()['beta_walk']\n",
    "\n",
    "beta1_bike=[]\n",
    "beta2_bike=[]\n",
    "beta3_bike=[]\n",
    "beta4_bike=[]\n",
    "beta5_bike=[]\n",
    "beta6_bike=[]\n",
    "beta7_bike=[]\n",
    "beta8_bike=[]\n",
    "beta9_bike=[]\n",
    "beta10_bike=[]\n",
    "beta11_bike=[]\n",
    "beta12_bike=[]\n",
    "beta13_bike=[]\n",
    "beta14_bike=[]\n",
    "beta15_bike=[]\n",
    "beta1_walk=[]\n",
    "beta2_walk=[]\n",
    "beta3_walk=[]\n",
    "beta4_walk=[]\n",
    "beta5_walk=[]\n",
    "beta6_walk=[]\n",
    "beta7_walk=[]\n",
    "beta8_walk=[]\n",
    "beta9_walk=[]\n",
    "beta10_walk=[]\n",
    "beta11_walk=[]\n",
    "beta12_walk=[]\n",
    "beta13_walk=[]\n",
    "beta14_walk=[]\n",
    "beta15_walk=[]\n",
    "\n",
    "# loop through the city-level variables and create arrays of all coefficient estimates\n",
    "for mode in ['bike','walk']:\n",
    "    for var in range(1,len(coefficient_dict)+1):\n",
    "        betas[f'beta{var}_{mode}']=np.asarray(betas[f'betas_{mode}'])[:,var-1,:].T\n",
    "\n",
    "# now these into dictionaries with the underlying variable name\n",
    "for mode in ['bike','walk']:\n",
    "    for key, value in coefficient_dict.items():\n",
    "        betas[f'{value}_{mode}']={f'{key}_{mode}':betas[f'{value}_{mode}']}\n",
    "        \n",
    "for mode in ['bike','walk']:\n",
    "    for value in coefficient_dict.values():\n",
    "        arviz.plot_trace(data=betas[f'{value}_{mode}'],chain_prop='color',figsize=(10, 4))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outputpath+'betas_{}_{}_{}.png'.format(value, mode, model_type), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf670b5-7821-4e77-9771-26ab812d4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773b872-057a-43e9-bcb1-32531a5869a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gammas = {}\n",
    "gammas['gammas_bike'] = combined_fit.stan_variables()['gamma_bike']\n",
    "gammas['gammas_walk'] = combined_fit.stan_variables()['gamma_walk']\n",
    "\n",
    "intercept_bike=[]\n",
    "sndi_bike=[]\n",
    "density_bike=[]\n",
    "density2_bike=[]\n",
    "precip_bike=[]\n",
    "min_temp_bike=[]\n",
    "min_temp2_bike=[]\n",
    "max_temp_bike=[]\n",
    "max_temp2_bike=[]\n",
    "bikelanes_bike=[]\n",
    "slope_bike=[]\n",
    "includes_inboundoutbound_bike=[]\n",
    "rail_in_city_bike=[]\n",
    "sndi_added_bike=[]\n",
    "population_bike=[]\n",
    "motorways_bike=[]\n",
    "\n",
    "intercept_walk=[]\n",
    "sndi_walk=[]\n",
    "density_walk=[]\n",
    "density2_walk=[]\n",
    "precip_walk=[]\n",
    "min_temp_walk=[]\n",
    "min_temp2_walk=[]\n",
    "max_temp_walk=[]\n",
    "max_temp2_walk=[]\n",
    "bikelanes_walk=[]\n",
    "slope_walk=[]\n",
    "includes_inboundoutbound_walk=[]\n",
    "rail_in_city_walk=[]\n",
    "sndi_added_walk=[]\n",
    "population_walk=[]\n",
    "motorways_walk=[]\n",
    "\n",
    "coeff_labels = ['Intercept','GDP per capita', 'Dependency ratio', 'Gasoline price']\n",
    "\n",
    "for mode in ['bike','walk']:\n",
    "    for city_var, i in city_var_order.items():\n",
    "        gammas[f'{city_var}_{mode}']=np.asarray(gammas[f'gammas_{mode}'])[:,i,:].T\n",
    "    \n",
    "# now do the coefficient plots!\n",
    "for mode in ['bike','walk']:\n",
    "    for key in city_var_order.keys():\n",
    "        ax=arviz.plot_trace(data=gammas[f'{key}_{mode}'],chain_prop='color',legend=True,figsize=(10, 4))\n",
    "        ax[0,0].legend(labels=['Intercept','','GDP per capita','','Dependency ratio','','Gasoline price'], fontsize=7, loc='upper right')\n",
    "        ax[0,0].set_title(f'{mode}_{key}')\n",
    "        ax[0,1].set_title(f'{mode}_{key}')\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(outputpath+'countrycoeffs_{}_{}_{}.pdf'.format(key, mode, model_type))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1687be6-cf9f-40a2-be2e-902806f7aad4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Extracting Estimates for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26eeca-931b-45a5-a687-17f1e7002c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_estimates=pd.DataFrame()\n",
    "\n",
    "# for beta model, we save marginal effects. for linear model, the actual beta coefficients\n",
    "cname = 'beta' if model_type=='linear' else 'me'\n",
    "for mode in ['bike','walk']:\n",
    "    # filter to just estimates of betas (city-level predictors)\n",
    "    betas_mode_summary=combined_summary[combined_summary.var_name.str.contains(f'{cname}_{mode}')].copy()\n",
    "    # unpack the country and variable\n",
    "    betas_mode_summary.var_name=betas_mode_summary.var_name.str.replace(f'{cname}_{mode}','')\n",
    "    if cname=='me':  # we need to average the marginal effect over country\n",
    "        n_vars = len(coefficient_dict)\n",
    "        betas_mode_summary[['country', 'beta']] = betas_mode_summary['var_name'].apply(lambda x: pd.Series(extract_values(x)))\n",
    "        betas_mode_summary['country']=np.repeat(city_data.country_num.values, n_vars)\n",
    "        betas_mode_summary = betas_mode_summary.groupby(['country','beta'])[['5%','50%','95%','Mean']].mean().reset_index()\n",
    "    else:\n",
    "        assert model_type=='linear'\n",
    "        betas_mode_summary[['beta','country']] = betas_mode_summary['var_name'].apply(lambda x: pd.Series(extract_values(x)))\n",
    "        # combined effect of linear and squared for min temp\n",
    "        me = combined_summary[combined_summary.var_name.str.contains(f'me_{mode}_mintemp')]\n",
    "        me['country'] = city_data.country_num.values + 1\n",
    "        me = me.groupby('country')[['5%','50%','95%','Mean']].mean().reset_index()\n",
    "        me['beta'] = 12  # variable number\n",
    "        betas_mode_summary = pd.concat([betas_mode_summary[betas_mode_summary.beta!=12], me])\n",
    "    betas_mode_summary['beta']=betas_mode_summary['beta'].apply(lambda x: 'beta'+str(x)+'_'+mode)\n",
    "    # reformat to be one row per city instead of one row per city-variable\n",
    "    beta_mode_pivot=pd.pivot_table(data=betas_mode_summary,values=['5%','50%','95%','Mean'],index='country',columns='beta',aggfunc='mean',fill_value=None)\n",
    "    new_columns = ['_'.join(map(str, reversed(col))) for col in beta_mode_pivot.columns]\n",
    "    beta_mode_pivot.columns=new_columns\n",
    "    beta_estimates=pd.concat([beta_estimates,beta_mode_pivot],axis=1)\n",
    "# replace country number with country name, 0-indexed\n",
    "beta_estimates=beta_estimates.reset_index().drop(columns=['country']).join(country_data.country).set_index('country')\n",
    "beta_estimates.to_csv(outputpath+'beta_estimates_'+model_type+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b498dc4-8be8-4e40-9033-15bfdd73040a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma_estimates=pd.DataFrame()\n",
    "\n",
    "cname = 'gamma' if model_type=='linear' else 'me_country'\n",
    "\n",
    "for mode in ['bike','walk']:\n",
    "    # filter to just estimates of gammas (country-level predictors)\n",
    "    gammas_mode_summary=combined_summary[combined_summary.var_name.str.contains(f'{cname}_{mode}')].copy()\n",
    "    # unpack the country and variable\n",
    "    gammas_mode_summary.var_name=gammas_mode_summary.var_name.str.replace(f'{cname}_{mode}','')\n",
    "    if cname=='me_country': # collapse to mean\n",
    "        gammas_mode_summary[['country_var', 'city']] = gammas_mode_summary['var_name'].apply(lambda x: pd.Series(extract_values(x)))\n",
    "        gammas_mode_summary.country_var=gammas_mode_summary.country_var.replace({1:'intercept',2:'gdp_per_capita',3:'dependency_ratio',4:'gas_prices'})+'_'+mode\n",
    "        gammas_mode_summary = gammas_mode_summary.groupby(['country_var'])[['5%','50%','95%','Mean']].mean()\n",
    "        gamma_estimates=pd.concat([gamma_estimates,gammas_mode_summary],axis=0)\n",
    "    else:\n",
    "        gammas_mode_summary[['city_var', 'country_var']] = gammas_mode_summary['var_name'].apply(lambda x: pd.Series(extract_values(x)))\n",
    "        gammas_mode_summary['city_var']=gammas_mode_summary['city_var'].apply(lambda x: 'beta'+str(x)+'_'+mode)   \n",
    "        gammas_mode_summary.country_var=gammas_mode_summary.country_var.replace({1:'intercept',2:'gdp_per_capita',3:'dependency_ratio',4:'gas_prices'})\n",
    "        gamma_mode_pivot=pd.pivot_table(data=gammas_mode_summary,values=['5%','50%','95%','Mean'],index='city_var',columns='country_var',aggfunc='mean',fill_value=None)\n",
    "        new_columns = ['_'.join(map(str, reversed(col))) for col in gamma_mode_pivot.columns]                                \n",
    "        gamma_mode_pivot.columns=new_columns\n",
    "        gamma_estimates=pd.concat([gamma_estimates,gamma_mode_pivot],axis=0)\n",
    "\n",
    "gamma_estimates=gamma_estimates.reset_index()\n",
    "gamma_estimates.to_csv(outputpath+'gamma_estimates_'+model_type+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358df46e-6cdc-4214-abe0-388902f1429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summary coefficients to a table, for inclusion in the SI, with column names\n",
    "median_row = int((len(beta_estimates)-1)/2) # assumes an odd number of countries\n",
    "outputdf = pd.DataFrame()\n",
    "if model_type=='linear':\n",
    "    gamma_estimates.set_index('city_var', inplace=True) # for easier accessing\n",
    "else:\n",
    "    gamma_estimates.set_index('country_var', inplace=True) # for easier accessing\n",
    "\n",
    "assert len(beta_estimates.columns)%4==0\n",
    "for mode in ['walk','bike']:\n",
    "    output_table = {}\n",
    "    for coeff in coefficient_dict:\n",
    "        colname = coefficient_dict[coeff]+'_'+mode+'_50%'\n",
    "        beta_estimates.sort_values(by=colname, inplace=True)\n",
    "        median_txt = '{:.3f}'.format(beta_estimates.iloc[median_row][colname])\n",
    "        UIcols = colname.replace('50%','5%'),colname.replace('50%','95%') \n",
    "        UI_txt = ' ({:.3f}, {:.3f})'.format(beta_estimates.iloc[median_row][UIcols[0]], beta_estimates.iloc[median_row][UIcols[1]])\n",
    "        coeffname = 'beta intercept' if coeff == 'intercept' else coeff\n",
    "        output_table[coeffname] = median_txt + UI_txt\n",
    "    \n",
    "    # add gamma coefficients, but only for intercept (not the interactions)\n",
    "    for coeff in ['dependency_ratio','gas_prices','gdp_per_capita', 'intercept']:\n",
    "        if model_type=='linear':\n",
    "            median_txt = '{:.3f}'.format(gamma_estimates.loc['beta1_'+mode,coeff+'_50%'])\n",
    "            UI_txt = ' ({:.3f}, {:.3f})'.format(gamma_estimates.loc['beta1_'+mode,coeff+'_5%'], gamma_estimates.loc['beta1_'+mode,coeff+'_95%']) \n",
    "        else:\n",
    "            median_txt = '{:.3f}'.format(gamma_estimates.loc[coeff+'_'+mode,'50%'])\n",
    "            UI_txt = ' ({:.3f}, {:.3f})'.format(gamma_estimates.loc[coeff+'_'+mode,'5%'], gamma_estimates.loc[coeff+'_'+mode,'95%'])\n",
    "        \n",
    "        coeffname = 'gamma intercept' if coeff == 'intercept' else coeff\n",
    "        output_table[coeffname] = median_txt + UI_txt\n",
    "        \n",
    "    outputdf[mode] = pd.Series(output_table)\n",
    "    \n",
    "print(outputdf)\n",
    "outputdf.to_csv(outputpath+'Coefficient_table_{}.csv'.format(model_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8702b039-9e88-48dc-936e-8feb47fe0fa7",
   "metadata": {},
   "source": [
    "### Bike lane scenarios\n",
    "\n",
    "Here, we use the fitted model to generate estimates for new scenarios with different quantities of bicycle lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b4f32-4c4e-4647-9a2f-8d68e3ebcdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_type in ['beta','linear'] # stop here if not\n",
    "tmpdir = '../tmpfns/'\n",
    "if not(os.path.exists(tmpdir)): os.mkdir(tmpdir)\n",
    "    \n",
    "# run generated quantities for displacement estimates\n",
    "# https://mc-stan.org/cmdstanpy/users-guide/examples/Run%20Generated%20Quantities.html\n",
    "model_gq = CmdStanModel(stan_file=stan_path+model_type+'_gq.stan')\n",
    "try: # use fit object\n",
    "    gq = model_gq.generate_quantities(data=hierarchical_data, previous_fit=combined_fit, gq_output_dir=tmpdir)\n",
    "except: # load in previous estimates\n",
    "    csv_files = ['stan_output/'+model_type+'/'+fn for fn in os.listdir('stan_output/'+model_type) if fn.endswith('.csv')]\n",
    "    print(csv_files)\n",
    "    assert len(csv_files)==4\n",
    "    gq = model_gq.generate_quantities(data=hierarchical_data, previous_fit=csv_files, gq_output_dir=tmpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec700de-5683-4225-a7df-edc753a82577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem is that the output files are massive (10GB per chain), and can't easily be read into memory\n",
    "# so let's read in line by line and write to temporary files, which we can then read in and calculate the median\n",
    "assert model_type in ['beta','linear'] # stop here if not\n",
    "   \n",
    "output_fns = ['tmp_gq_{}_{}'.format(model_type, ii) for ii in range(1,301)]\n",
    "output_files = [open(tmpdir+fn,'w') for fn in output_fns]\n",
    "gq_fns = [fn for fn in os.listdir(tmpdir) if fn.startswith(model_type+'_gq') and fn.endswith('.csv')]\n",
    "assert len(gq_fns)==4\n",
    "\n",
    "# read in all 4 chains, and write to 100 different files\n",
    "headerrow = '',''\n",
    "for chain, gq_fn in enumerate(gq_fns):\n",
    "    print('Processing chain {} and file {}'.format(chain+1, gq_fn))\n",
    "    infile = open(tmpdir+gq_fn,'r')\n",
    "    # get past header\n",
    "    while 'aas_w.' not in headerrow:\n",
    "        headerrow = infile.readline()\n",
    "    # check file structure - aas should start at column 3\n",
    "    headerrow = headerrow.split(',')\n",
    "    assert 'aas' not in headerrow[0]+headerrow[1] and np.all(['aas' in r for r in headerrow[2:]])\n",
    "    n_cities = int(headerrow[-1].split('.')[1])\n",
    "    assert n_cities==11587\n",
    "    assert len(headerrow)==n_cities*3*100+2 # 3 variables, 100 scenarios per city, 2 extra cols\n",
    "\n",
    "    # check that modes are in right order\n",
    "    for i, m in enumerate(['w','b','c']):\n",
    "        startidx, stopidx = 2+n_cities*100*i, 2+n_cities*100*(i+1)\n",
    "        assert all([ii[:5] == 'aas_'+m for ii in headerrow[startidx:stopidx]])\n",
    "    \n",
    "    counter = 0\n",
    "    while infile:\n",
    "        row = infile.readline()\n",
    "        if row=='': break # end of data\n",
    "        row = row.split(',')\n",
    "        counter+=1\n",
    "        for i, outfile in enumerate(output_files):\n",
    "            line_to_write = row[2+i*n_cities:2+(i+1)*n_cities] \n",
    "            assert len(line_to_write)==n_cities\n",
    "            outfile.write(','.join(line_to_write)+'\\n')\n",
    "    infile.close()\n",
    "    assert counter==1000 # we had 1000 sampling iterations per chain\n",
    "    print('Processed {} rows from chain {}'.format(counter, chain+1))\n",
    "for f in output_files: f.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075f464-febb-49de-8631-b40d19327e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in city data and process it\n",
    "# we need this to join to the simulations to calculate CO2 savings\n",
    "assert model_type in ['beta','linear']\n",
    "exclude_transit = False\n",
    "fn_suffix='_excltransit' if exclude_transit else ''  # save files under different name\n",
    "    \n",
    "city_data=pd.read_csv('data/data_23.csv', index_col=0).reset_index()\n",
    "if exclude_transit:\n",
    "    # don't include transit in displaced mode share. The easiest way to do this is to set transit mode share to zero\n",
    "    city_data.km_transit=0\n",
    "    \n",
    "efs = pd.read_csv('data/transport.csv')\n",
    "ef_col=[col for col in efs.columns.to_list() if '_emissions_factor_kg_co2e_per_liter' in col or 'fuel_efficiency_km_per_liter' in col]\n",
    "efs=efs[['feature_id','year']+ef_col].copy()\n",
    "efs = efs[efs.year==2023]\n",
    "city_data=city_data.set_index('feature_id').join(efs.set_index('feature_id')).reset_index()\n",
    "\n",
    "for mode in ['automobile','motorcycle','bus','ferry','rail','subway','tram','on_foot','cycling']:\n",
    "    city_data[f'{mode}_occupancy'] = 9.9 if mode=='bus' else 1.7 if mode=='automobile' else 1.0\n",
    "\n",
    "# impute missing data by replacing with country values\n",
    "for mode in ['automobile','motorcycle','bus','ferry','rail','subway','tram']:\n",
    "    for col in ['_emissions_factor_kg_co2e_per_liter', '_fuel_efficiency_km_per_liter']:\n",
    "        # https://stackoverflow.com/questions/19966018/filling-missing-values-by-mean-in-each-group\n",
    "        city_data.loc[city_data[mode+col]==0, mode+col] = np.nan\n",
    "        city_data[mode+col] = city_data.groupby('country')[mode+col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# a few missing for bus, so replace with the global mean\n",
    "bus_mean = city_data.bus_fuel_efficiency_km_per_liter.mean()\n",
    "city_data.loc[city_data.km_bus>0] = city_data.loc[city_data.km_bus>0].fillna(bus_mean)\n",
    "\n",
    "# impute remaining missing data (all MC) by using the global proportion of MC fuel economy to car\n",
    "fe_ratio = city_data.groupby('country').automobile_fuel_efficiency_km_per_liter.mean().mean() / city_data.groupby('country').motorcycle_fuel_efficiency_km_per_liter.mean().mean()\n",
    "city_data.loc[pd.isnull(city_data.motorcycle_fuel_efficiency_km_per_liter), 'motorcycle_fuel_efficiency_km_per_liter'] = city_data.automobile_fuel_efficiency_km_per_liter / fe_ratio \n",
    "\n",
    "# calculate to emissions per km, and displaced emissions per km of walk/bike\n",
    "# note that emissions are already converted to PASSENGER KM \n",
    "for mode in ['automobile','motorcycle']:\n",
    "    city_data[mode+'_kg_co2_e_per_km'] = city_data[mode+'_emissions_factor_kg_co2e_per_liter'] / city_data[mode+'_fuel_efficiency_km_per_liter'] / city_data[mode+'_occupancy']\n",
    "    city_data[mode+'_km_share_fordisplacement'] = city_data['km_'+mode] / (city_data.km_automobile + city_data.km_motorcycle + city_data.km_transit)\n",
    "\n",
    "# for transit, assume global average for rail - 22.35 kgCO2 / pkm \n",
    "# https://www.iea.org/energy-system/transport/rail\n",
    "city_data['transit_kg_co2_e_per_km'] = (\n",
    "    city_data['bus_emissions_factor_kg_co2e_per_liter'] / city_data['bus_fuel_efficiency_km_per_liter'] / city_data['bus_occupancy']\n",
    "         * (city_data.km_bus / city_data.km_transit)\n",
    "  +  0.02235 * (1- city_data.km_bus / city_data.km_transit))\n",
    "city_data['transit_km_share_fordisplacement'] = city_data.km_transit / (city_data.km_automobile + city_data.km_motorcycle + city_data.km_transit)\n",
    "\n",
    "# check imputation worked. this allows us to use fillna a couple of lines below\n",
    "assert np.all(pd.notnull(city_data[city_data.motorcycle_km_share_fordisplacement>0]).motorcycle_kg_co2_e_per_km)\n",
    "\n",
    "city_data['kg_co2_displaced_per_active_km'] = city_data.automobile_km_share_fordisplacement * city_data.automobile_kg_co2_e_per_km + (city_data.motorcycle_km_share_fordisplacement * city_data.motorcycle_kg_co2_e_per_km).fillna(0)\n",
    "city_data['kg_co2_displaced_per_active_km_including_transit_emissions'] = (city_data.kg_co2_displaced_per_active_km + \n",
    "    (city_data.transit_km_share_fordisplacement * city_data.transit_kg_co2_e_per_km).fillna(0))\n",
    "city_data['kg_co2_auto_mc'] = city_data.km_automobile * city_data.automobile_kg_co2_e_per_km + (city_data.km_motorcycle * city_data.motorcycle_kg_co2_e_per_km).fillna(0)\n",
    "city_data['kg_co2_auto_mc_transit'] = city_data['kg_co2_auto_mc'] + (city_data.km_transit * city_data.transit_kg_co2_e_per_km).fillna(0)\n",
    "\n",
    "assert np.all(pd.notnull(city_data['kg_co2_displaced_per_active_km']))\n",
    "assert np.all(pd.notnull(city_data['kg_co2_displaced_per_active_km_including_transit_emissions']))\n",
    "assert np.all(pd.notnull(city_data['kg_co2_auto_mc']))\n",
    "assert np.all(pd.notnull(city_data['kg_co2_auto_mc_transit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29873dc-31bf-43b4-8e66-0085758d5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, read all the files in and calculate percentiles\n",
    "# each file has all the data for one simulation (1-100) of the bike lanes variable, \n",
    "#     for walk, bike or combined\n",
    "assert model_type in ['beta','linear'] # stop here if not\n",
    "\n",
    "colnames = [str(ii) for ii in range(1,n_cities+1)]\n",
    "\n",
    "# store global aggregates for each simulation\n",
    "agg_medians, agg_pc_025, agg_pc_975 = {}, {}, {}\n",
    "\n",
    "for j, mode in enumerate(['_walk','_bike','_combined']):\n",
    "    output_fns_subset = output_fns[j*100:(j+1)*100]\n",
    "    means, medians, pc_025, pc_975  = [], [], [], []\n",
    "    agg_medians[mode], agg_pc_025[mode], agg_pc_975[mode] = {}, {}, {}\n",
    "    for i, fn in enumerate(output_fns_subset):\n",
    "        bl = i+1 # bike lane simulated variable starts at 1\n",
    "        agg_medians[mode][bl], agg_pc_025[mode][bl], agg_pc_975[mode][bl] = {}, {}, {}\n",
    "        # dataframe is N iterations x J cities, i.e. 4000 x 11587\n",
    "        df = pd.read_csv(tmpdir+fn, header=None, names=colnames, dtype='float32')\n",
    "        means.append(df.mean())\n",
    "        medians.append(df.median())\n",
    "        pc_025.append(df.quantile(q=0.025, axis=0))\n",
    "        pc_975.append(df.quantile(q=0.975, axis=0))\n",
    "        \n",
    "        # we also calculate aggregate CO2 savings here, so that we can get the uncertainty bands\n",
    "        df = df.T.reset_index(drop=True) # transpose so we can join to city_data\n",
    "        co2_kg = df.mul(city_data.km_no_transit, axis=0).mul(city_data.kg_co2_displaced_per_active_km, axis=0).sum()\n",
    "        # also multiple by km_notransit (because that's how the model is estimated), but include transit emissions in the displacement\n",
    "        co2_kg_inctransit = df.mul(city_data.km_no_transit, axis=0).mul(city_data.kg_co2_displaced_per_active_km_including_transit_emissions, axis=0).sum()\n",
    "        pc_reduction = co2_kg / city_data.kg_co2_auto_mc.sum()       \n",
    "        pc_reduction_inctransit = co2_kg_inctransit / city_data.kg_co2_auto_mc_transit.sum()\n",
    "        added_km = df.mul(city_data.km_no_transit, axis=0).sum()\n",
    "        # compute medians and percentiles for aggregates\n",
    "        for dfname in ['co2_kg', 'pc_reduction', 'added_km','co2_kg_inctransit','pc_reduction_inctransit']:\n",
    "            tmpdf = globals()[dfname]\n",
    "            assert len(tmpdf)==4000\n",
    "            agg_medians[mode][bl][dfname] = tmpdf.median()\n",
    "            agg_pc_025[mode][bl][dfname]  = tmpdf.quantile(q=0.025)\n",
    "            agg_pc_975[mode][bl][dfname]  = tmpdf.quantile(q=0.975)\n",
    "\n",
    "        del df # save memory\n",
    "\n",
    "    for fn, adf in zip(['means','medians','pc_025','pc_975'], [means, medians, pc_025, pc_975]):\n",
    "        cdf = pd.concat(adf,axis=1)\n",
    "        cdf.columns=[str(ii) for ii in range(1,101)]\n",
    "        outfn = outputpath+'gq_'+model_type+'_'+fn+mode+fn_suffix+'.csv'\n",
    "        cdf.to_csv(outfn)\n",
    "        print('Saved '+outfn)\n",
    "\n",
    "# save aggregated\n",
    "for fn, adict in zip(['medians','pc_025','pc_975'], [agg_medians, agg_pc_025, agg_pc_975]):\n",
    "    adf = pd.concat({k: pd.DataFrame(v).T for k, v in adict.items()}, axis=0)\n",
    "    adf.index.names=['mode','bikelanes']\n",
    "    outfn = outputpath+'gq_'+model_type+'_'+fn+'_aggregated'+fn_suffix+'.csv'\n",
    "    adf.to_csv(outfn)\n",
    "    print('Saved '+outfn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fd963-d8d2-439f-ad1c-908b62b659f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now delete tmpfiles and gq output (because that's 10GB+!)\n",
    "# do this after running both exclude_transit =False and =True\n",
    "gq_fns = [fn for fn in os.listdir(tmpdir) if fn.startswith(model_type+'_gq') or fn=='.DS_Store'] # including the non-csv files\n",
    "for fn in output_fns+gq_fns:\n",
    "    os.remove(tmpdir+fn)\n",
    "os.rmdir(tmpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e395a-4118-497b-aef6-81243ce10b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 0 and model_type=='linear':  # deprecated\n",
    "    # extract just the projected additional active shares at different bikelane scenarios\n",
    "    additional_active_share=combined_summary[combined_summary.var_name.str.contains('additional_active_share')].copy()\n",
    "    additional_active_share.var_name=additional_active_share.var_name.str.replace('additional_active_share','')\n",
    "    additional_active_share[['city', 'bikelane_scenario']] = additional_active_share['var_name'].apply(lambda x: pd.Series(extract_values(x)))\n",
    "    additional_active_share_summary=pd.pivot_table(data=additional_active_share,values=['5%','50%','95%','Mean'],index='city',columns='bikelane_scenario',aggfunc='mean',fill_value=None)\n",
    "    new_columns = ['bikelanes_'+'_'.join(map(str, reversed(col))) for col in additional_active_share_summary.columns]       \n",
    "    additional_active_share_summary.columns=new_columns\n",
    "    # replace city number with feature_id\n",
    "    additional_active_share_summary=additional_active_share_summary.reset_index().join(city_data.feature_id).drop(columns=['city']).set_index('feature_id')\n",
    "    additional_active_share_summary.to_csv(outputpath'additional_active_share_'+model_type+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
